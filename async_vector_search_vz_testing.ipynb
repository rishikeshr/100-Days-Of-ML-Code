{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rishikeshr/100-Days-Of-ML-Code/blob/master/async_vector_search_vz_testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade --quiet langchain-elasticsearch langchain-openai tiktoken langchain pandas\n"
      ],
      "metadata": {
        "id": "zddKzPoHLoyb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9813d757-6827-4b6b-c9b4-7761c2cca917"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/52.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.0/52.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m997.8/997.8 kB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m84.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m523.3/523.3 kB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.9/394.9 kB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.9/362.9 kB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.1/149.1 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.4/64.4 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m533.4/533.4 kB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pandas<2.2.2dev0,>=2.0, but you have pandas 2.2.2 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.0.3, but you have pandas 2.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from elasticsearch import AsyncElasticsearch, OrjsonSerializer\n",
        "\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "import concurrent.futures\n",
        "import time\n",
        "import random\n",
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "import aiohttp\n",
        "from elasticsearch import Elasticsearch\n",
        "import configparser\n",
        "import asyncio\n",
        "import urllib.request\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import statistics\n",
        "\n",
        "# embeddings=[]\n",
        "# for i in range(100):\n",
        "#     # random_array = np.random.normal(0.0, 1.0, 768)\n",
        "#     v = np.random.rand(1, 768)\n",
        "#     vn = v / np.linalg.norm(v, axis=1, keepdims=True)\n",
        "#     # print(vn)\n",
        "#     # break\n",
        "#     embeddings.append(vn.tolist()[0])\n",
        "\n",
        "# with open(\"new_embedding_file_100k_random.txt\",\"w\") as fp:\n",
        "#     json.dump(embeddings,fp)\n",
        "\n",
        "# with open('new_embedding_file_100k_random.txt') as f:\n",
        "#   vector_lines = json.load(f)\n",
        "\n",
        "with open('new_embedding_file_100k_random.txt') as f:\n",
        "   vector_lines = json.load(f)\n",
        "\n",
        "# Set up Elasticsearch connection\n",
        "ES_HOSTS = [\"https://34.29.60.210:9200\"]  # Replace with your Elasticsearch host(s)\n",
        "ELASTIC_API_KEY = \"OGRLVGk1RUI5b2xjak1rbDBMU1I6TDd2enlLUERUMG01dUhfbHJscjFtUQ==\"\n",
        "\n",
        "# Initialize the AsyncElasticsearch client\n",
        "es = AsyncElasticsearch(\n",
        "    hosts=ES_HOSTS,\n",
        "    api_key=ELASTIC_API_KEY,\n",
        "    verify_certs=False,\n",
        "    retry_on_timeout=True,\n",
        "    #http_compress=True,\n",
        "    connections_per_node=200,\n",
        "    serializers={\"application/json\": OrjsonSerializer()}\n",
        ")\n",
        "\n",
        "async def run_knn_search(query_vector, index_name, field_name):\n",
        "    search_body = {\n",
        "        \"_source\": False, ##to avoid returning full doc\n",
        "        \"size\": 4,\n",
        "        \"terminate_after\": 5,\n",
        "        \"knn\": {\n",
        "            \"field\": field_name,\n",
        "            \"query_vector\": query_vector,\n",
        "            \"k\": 4,\n",
        "            \"num_candidates\": 5\n",
        "        }\n",
        "    }\n",
        "\n",
        "    start_service_time = time.time()  # Start measuring service time\n",
        "    response = await es.search(index=index_name, body=search_body)\n",
        "    end_service_time = time.time()  # End measuring service time\n",
        "\n",
        "    took_time = response['took']  # Time in milliseconds\n",
        "    service_time = end_service_time - start_service_time  # Service time in seconds\n",
        "\n",
        "    return took_time, service_time  # Return both took time and service time\n",
        "\n",
        "async def thread_task(index_name, field_name, vectors_for_thread):\n",
        "    took_times = []\n",
        "    service_times = []\n",
        "\n",
        "    num_queries = len(vectors_for_thread)\n",
        "    for i in range(num_queries):\n",
        "        query_vector = vectors_for_thread[i % len(vectors_for_thread)]  # Reuse vectors if needed\n",
        "        took_time, service_time = await run_knn_search(query_vector, index_name, field_name)\n",
        "        took_times.append(took_time)\n",
        "        service_times.append(service_time)\n",
        "\n",
        "    return took_times, service_times\n",
        "\n",
        "async def main():\n",
        "    num_threads = 100  # Number of parallel threads (tasks) ex: 100\n",
        "    queries_per_thread = 100  # Number of queries each thread will run ex: 1000\n",
        "    index_name = \"tech15m2_random\"\n",
        "    field_name = \"vector\"\n",
        "\n",
        "    try:\n",
        "        # Get the total number of documents in the index\n",
        "        count_response = await es.count(index=index_name)\n",
        "        total_docs = count_response['count']\n",
        "\n",
        "        # Output the total number of documents in the index\n",
        "        print(f\"Total number of documents in the index '{index_name}': {total_docs}\")\n",
        "\n",
        "        # Get index settings to retrieve the number of shards and replicas\n",
        "        index_settings = await es.indices.get(index=index_name)\n",
        "        settings = index_settings[index_name]['settings']['index']\n",
        "        number_of_shards = settings['number_of_shards']\n",
        "        number_of_replicas = settings['number_of_replicas']\n",
        "\n",
        "        # Output the number of primary shards and replicas\n",
        "        print(f\"Number of primary shards: {number_of_shards}\")\n",
        "        print(f\"Number of replicas: {number_of_replicas}\")\n",
        "\n",
        "        # If total_queries exceeds num_vectors, vectors will be reused\n",
        "        vectors_per_thread = [vector_lines[i % len(vector_lines)] for i in range(queries_per_thread * num_threads)]\n",
        "\n",
        "        # Measure total runtime\n",
        "        start_time = time.time()  # Start time for total runtime\n",
        "\n",
        "        # Assign vectors to threads\n",
        "        tasks = []\n",
        "        for thread_index in range(num_threads):\n",
        "            start = thread_index * queries_per_thread\n",
        "            end = start + queries_per_thread\n",
        "            vectors_for_thread = vectors_per_thread[start:end]\n",
        "            tasks.append(thread_task(index_name, field_name, vectors_for_thread))\n",
        "\n",
        "        # Gather the results from all tasks\n",
        "        all_results = await asyncio.gather(*tasks)\n",
        "\n",
        "        end_time = time.time()  # End time for total runtime\n",
        "        total_runtime = end_time - start_time  # Total runtime in seconds\n",
        "\n",
        "        # Flatten the lists of took_times and service_times, and convert took_times to seconds\n",
        "        took_times_flat = [time / 1000.0 for sublist in [result[0] for result in all_results] for time in sublist]\n",
        "        service_times_flat = [time for sublist in [result[1] for result in all_results] for time in sublist]\n",
        "\n",
        "        min_took_time = min(took_times_flat)\n",
        "        max_took_time = max(took_times_flat)\n",
        "        avg_took_time = statistics.mean(took_times_flat)\n",
        "\n",
        "        min_service_time = min(service_times_flat)\n",
        "        max_service_time = max(service_times_flat)\n",
        "        avg_service_time = statistics.mean(service_times_flat)\n",
        "\n",
        "        # Calculate total number of queries\n",
        "        total_queries_ran = len(took_times_flat)\n",
        "\n",
        "        # Calculate average response time\n",
        "        avg_response_time = total_runtime / total_queries_ran\n",
        "\n",
        "        # Print the timing statistics in seconds\n",
        "        print(f\"Min took time (seconds): {min_took_time:.3f}\")\n",
        "        print(f\"Max took time (seconds): {max_took_time:.3f}\")\n",
        "        print(f\"Avg took time (seconds): {avg_took_time:.3f}\")\n",
        "\n",
        "        print(f\"Min service time (seconds): {min_service_time:.3f}\")\n",
        "        print(f\"Max service time (seconds): {max_service_time:.3f}\")\n",
        "        print(f\"Avg service time (seconds): {avg_service_time:.3f}\")\n",
        "\n",
        "        # Print the average response time\n",
        "        print(f\"Avg response time (seconds): {avg_response_time:.3f}\")\n",
        "\n",
        "        # Output the total number of queries ran\n",
        "        print(f\"Total number of queries ran: {total_queries_ran}\")\n",
        "\n",
        "        # Output the total runtime\n",
        "        print(f\"Total runtime (seconds): {total_runtime:.3f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "# Run the async function\n",
        "#await main()\n",
        "\n",
        "# Ensure the client is properly closed after main completes\n",
        "    await es.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(main())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9C4aWaYUlY-8",
        "outputId": "cb05a5b9-3b9e-4b97-e961-998b32b2b011"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of documents in the index 'tech15m2_random': 15610000\n",
            "Number of primary shards: 3\n",
            "Number of replicas: 1\n",
            "Min took time (seconds): 0.004\n",
            "Max took time (seconds): 0.241\n",
            "Avg took time (seconds): 0.015\n",
            "Min service time (seconds): 0.135\n",
            "Max service time (seconds): 0.795\n",
            "Avg service time (seconds): 0.157\n",
            "Avg response time (seconds): 0.002\n",
            "Total number of queries ran: 10000\n",
            "Total runtime (seconds): 16.539\n"
          ]
        }
      ]
    }
  ]
}